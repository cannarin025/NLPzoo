{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-summarisation-ProphetNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrAcLcehY9ZPCeNCwjFLgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umerhasan17/NLPzoo/blob/master/text_summarisation_ProphetNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SopYa7V56j39",
        "colab_type": "text"
      },
      "source": [
        "# ProphetNet\n",
        "\n",
        "ProphetNet is a Seq2Seq pre-training model`. The model is optimized by **n-step ahead prediction** aiming to predict the next n tokens simultaneously. This is an alternative to the traditional **1-step** Seq2Seq model. Modelling for n-steps prevents overfitting on 'strong local correlations' by encouraging planning for future tokens.\n",
        "\n",
        "# What are pre-trained models?\n",
        "\n",
        "A model that is trained to solve a problem similar to the one we want to solve on a large benchmark dataset. This can take the form of labeled data or unlabeled data with 'specific self-supervised objectives'. Pre-trained models are then 'fine-tuned to adapt to downstream tasks.'\n",
        "\n",
        "**Key terms to investigate:** Autoregressive language modelling, teacher forcing, bigram combination, greedy decoding, beam search.\n",
        "\n",
        "# Implementation Details\n",
        "\n",
        "**The Objective**\n",
        "\n",
        "**N-Stream Self-Attention Mechanism**\n",
        "\n",
        "* ProphetNet contains **n-stream self-attention** models with the main stream being the same as the self-attention in the original Transformer. \n",
        "* The parameters of the main stream are shared with every other predicting stream, this allows us to disable the n-stream during inference and only predict the next token. \n",
        "* Weights are assigned to loss values for all streams (giving higher weight to closer tokens is similar to the discount factor of future reward in reinforcement learning).\n",
        "\n",
        "**Modified positional embedding**\n",
        "\n",
        "**Mask based auto-encoder denoising task for Seq2Seq pre-training**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E6LgsCd54PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}